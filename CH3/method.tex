\chapter{Method} \label{method}
The first step of constructing a classifier for significant news is to acquire a dataset containing the labels for significant and non-significant news. Once the dataset is obtained, the texts are then processed to obtain the unigram terms. The IDF values for the terms are then calculated by using the Equation \eqref{eq:idf_equation}, which are then used to compute the TF-IDF vectors for the text.

The vectors for documents are then randomly divided into a training set and a test set. The training set is used to train the models and optimize the hyperparamters by using a cross validation technique. And, the test set is used to evaluate the performance of the best performing models from the cross validation step. The steps that were followed in order to train, validate, and test the classifier models are shown in \figureref{fig:training}.

\begin{figure}[h]
    \cfig{3}{training.png}{3}
    \caption{Flow diagram for training and testing}
    \label{fig:training}
\end{figure}

Once the models are trained, they can be used to predict the class assignment for new articles. The first step would be convert the  text to a vector by using the TF-IDF algorithm. Then, the vector can be used with a trained classifier model to predict the class label. The steps of the classification are summarized in \figureref{fig:prediction}.

\begin{figure}[h]
    \cfig{3}{prediction.png}{3}
    \caption{Flow diagram for predicting class label using pre-trained model}
    \label{fig:prediction}
\end{figure}

\section{Dataset}
We have provided a new definition of significant news for our purpose. Due to this reason, a dataset that matches categorization of text into significant and non-significant news does not exist. This prompted the creation of a new dataset for significant news detection. 

\subsection{Data Acquisition}
Rather than checking messages of general users to create the dataset, we focused on official law enforcement accounts. Those accounts would at least have some significant news besides some non-significant news.
The initial unlabelled dataset was obtained from the posts of Twitter users shown in \tableref{tbl:twitter_users}, with the help of python-oauth2\footnote{\url{https://github.com/joestump/python-oauth2}} library and official Twitter API\footnote{\url{https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html}}. The data was then stored locally in an SQLite\footnote{\url{https://www.sqlite.org/index.html}} database with the help of peewee\footnote{\url{http://docs.peewee-orm.com/en/latest/}} python library. A sample of data obtained from Twitter is as follows,

\begin{verbatim}
Watch: @PIX11News  gives an inside look at our #NeighborhoodPolicing
meetings on how they are connecting local NYPD police officers with
the community. https://t.co/D6K5DWxWWm https://t.co/NM0AWpPgja
\end{verbatim}

\begin{table}
    \centering
    \caption{Twitter accounts used for collecting data}
    \label{tbl:twitter_users}
    \begin{tabular}{c|c}
    \toprule
    Account & Link \\
    \midrule
    NYPD NEWS & \url{https://twitter.com/nypdnews} \\
    Metropolitan Police & \url{https://twitter.com/metpoliceuk} \\
    Victoria Police & \url{https://twitter.com/VictoriaPolice} \\
    Seattle Police Dept & \url{https://twitter.com/SeattlePD} \\
    NYPDCounterterrorism & \url{https://twitter.com/NYPDCT} \\
    \bottomrule
    \end{tabular}
\end{table}



\subsection{Labelling}
Each article was then manually labeled as either significant news or non-significant news based on the pre-conditions. The statistics of the new dataset constructed is shown in  \tableref{tbl:dataset_statistics} and some examples of the labels and their explanation are given below.

\begin{table}[h]
\begin{center}
\caption{Significant news dataset statistics}
\label{tbl:dataset_statistics}
\begin{tabular}{lr}
\toprule 
Label&Number of samples\\
\midrule 
Significant&1548\\
Non-significant&595\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsubsection{Labelling for Sample 1}
\textbf{Text:}
\textit{Great news!! Missing 20-year-old Melissa Wilson has been located safe and well. Thank you for the shares!}\par
\textbf{Label:} Significant\par
\textbf{Explanation:} A number of people might still be looking for the missing person either actively or inactively. Any updates to the whereabouts of the person would be an important piece of information for those people. The text mentions that the missing person has been found and is safe and well. If the missing person has in fact been located, the people searching for the person could stop searching for the person and can continue with their other tasks. Since the content may affect a large number of people, changes their routines of their daily life, and needs to be verified, it would be labelled as significant.

\subsubsection{Labelling for Sample 2}
\textbf{Text:}
\textit{Detectives investigating the murder of Kwabena Nelson in Tottenham have made an arrest Haringey.}\par
\textbf{Label:} Significant\par
\textbf{Explanation:} A murder of a person induces fear in a number of people who might be living in the same location where the crime is supposed to have occurred. The people living in the area might avoid walking alone in the neighbourhood at night, cancel their plans of moving in an apartment near the area or even move out of the area after knowing about it. Knowing that the detectives made an arrest on the crime would be a very important piece information that might bring them some peace of mind. As the content affect a large number of people, changes the routines of their daily life and needs to be verified, it would be labelled as significant.

\subsubsection{Labelling for Sample 3}
\textbf{Text:}
\textit{Great example of NYPDconnecting in the Bronx. NYPD49Pct Neighbor- hoodPolicing officers worked with the community to address a garbage condition on a resident block in their neighborhood.}\par
\textbf{Label:} Non-Significant\par
\textbf{Explanation:} The number of people the text could concern would be physically limited to the people living around that neighborhood. And, even for those people, addressing a garbage condition would be a minor concern. The information that the problem was resolved could hardly change their behavior. For most of them, their life would continue the same way even after knowing about it. As the number of people that the text affects is small and could hardly change their daily routine, the text should be labelled as non-significant.

\subsubsection{Labelling for Sample 4}
\textbf{Text:}
\textit{It was over before it began for this 18-year-old who lost her licence after only two hours.}\par
\textbf{Label:} Non-Significant\par
\textbf{Explanation:} The information in the content is of a personal nature which most of the time is essential only for immediate family members and friends. And, even for those people, the information that the person lost their licence could hardly be a matter of great importance as long as the person is safe. Since the content does not describe any unfortunate events, the only new information would be that the person lost their licence. For most of the immediate family and friends their life would continue the same way as before even after reading about the news. Since it fails to affect the daily routines of a large number of people it should be labelled as non-significant.

% Text Processing
\section{Feature Generation}
\subsection{Tokenization}
The tokenization is done with the help of NLTK\cite{loper2002nltk} python library which discards everything except English alphabets.  Tokenization done in this way will also discard the numeric characters such as "1", "2", "3", etc.  The tokenizer first removes all of the punctuation symbols from the text and then separates the words by using the white space characters. Furthermore, a regular expression was used to remove the URLs from the text. An example of the output generated by the tokenizer is shown in \tableref{tbl:tokenization_sample}

\begin{table}[h]
\begin{center}
\caption{Sample of tokenization}
\label{tbl:tokenization_sample}
\begin{tabular}{p{8cm}p{5cm}}
\toprule 
Text&Tokens\\
\midrule 
Watch: @PIX11News  gives an inside look at our \#NeighborhoodPolicing
meetings on how they are connecting local NYPD police officers with
the community. \url{https://t.co/D6K5DWxWWm} \url{https://t.co/NM0AWpPgja} & "watch", "pix", "news", "gives", "an", "inside", "look", "at", "our", "neighborhoodpolicing", "meetings", "on", "how", "they", "are", "connecting", "local", "nypd", "police", "officers", "with", "the", "community" \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Stop word filtering}
The NLTK library contains lists of stop words for English text that can be used to filter the tokens. For each token, a comparison is made with stop words one at a time and if a match is found the token is discarded from the feature set. \tableref{tbl:stop_words} shows the list of stop word present in the NLTK library.

\begin{table}
\begin{center}
\caption{English stopwords in nltk}
\label{tbl:stop_words}
\begin{tabular}{lcccccr}
\toprule
i&me&my&myself&we&our&ours\\ \hline 
ourselves&you&you're&you've&you'll&you'd&your\\ \hline 
yours&yourself&yourselves&he&him&his&himself\\ \hline 
she&she's&her&hers&herself&it&it's\\ \hline 
its&itself&they&them&their&theirs&themselves\\ \hline 
what&which&who&whom&this&that&that'll\\ \hline 
these&those&am&is&are&was&were\\ \hline 
be&been&being&have&has&had&having\\ \hline 
do&does&did&doing&a&an&the\\ \hline 
and&but&if&or&because&as&until\\ \hline 
while&of&at&by&for&with&about\\ \hline 
against&between&into&through&during&before&after\\ \hline 
above&below&to&from&up&down&in\\ \hline 
out&on&off&over&under&again&further\\ \hline 
then&once&here&there&when&where&why\\ \hline 
how&all&any&both&each&few&more\\ \hline 
most&other&some&such&no&nor&not\\ \hline 
only&own&same&so&than&too&very\\ \hline 
s&t&can&will&just&don&don't\\ \hline 
should&should've&now&d&ll&m&o\\ \hline 
re&ve&y&ain&aren&aren't&couldn\\ \hline 
couldn't&didn&didn't&doesn&doesn't&hadn&hadn't\\ \hline 
hasn&hasn't&haven&haven't&isn&isn't&ma\\ \hline 
mightn&mightn't&mustn&mustn't&needn&needn't&shan\\ \hline 
shan't&shouldn&shouldn't&wasn&wasn't&weren&weren't\\ \hline 
won&won't&wouldn&wouldn't\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

In addition to the stop words found in the NLTK library, the words shown in \tableref{tbl:extra_stop_words} were also added to the stop word list. The set of words such as "nypdpsa", "npyd", "nypdct", "seattledot", etc were added to the list to eliminate the bias that those words might introduce during the training. The other words that were added to the stop word list are frequently occurring words such as such as "one", "two", "Sunday", "Monday", "January", "February" whose presence in the text most of the time does not affect whether the text is significant or non-significant. Nevertheless, there could be some cases where temporal context might be important and some of these words could affect classification.

\begin{table}
\begin{center}
\caption{Extra stop words}
\label{tbl:extra_stop_words}
\begin{tabular}{lcccr}
\toprule
nypdprotecting&nypdconnecting&nypd&nypdoneil\\ \hline 
nypdpsa&nypdchiefofdept&nypdbklynnorth&nypdtransit\\ \hline 
nypddetect&nypdnews&nypdspecialops&nypdchiefpatrol\\ \hline 
nypdhighway&nypdtransport&nycsafenypdorg&nypdct\\ \hline 
nypdcentralpark&nycoem&nypddetectives&tcsnycmarathon\\ \hline 
sdnynews&newyorkfbi&nypdoneill&neighborhoodpolicing\\ \hline 
timessquare&victoria&bronx&sunday\\ \hline 
monday&tuesday&wednesday&thursday\\ \hline 
friday&saturday&seattlepd&seattledot\\ \hline 
seattlefir&seattle&brooklyn&manhattan\\ \hline 
one&two&three&four\\ \hline 
five&six&vicpolicenews&victoriapolice\\ \hline 
jointhemet&seattlefir&metpoliceuk&mpswestminster\\ \hline 
london&londonfire&richmond&nycgov\\ \hline 
https&mayorjenny&nyc&nycdot\\ \hline 
january&february&march&april\\ \hline 
may&june&july&august\\ \hline 
september&october&november&december\\ \hline 
joseph&melbourne\\

\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Stemming}
The Porter's algorithm\cite{porter1980algorithm} is a popular stemming algorithm that can reduce the inflected words to their stems. It does so by removing the suffixes from a inflected word using production rules such as the ones shown in \tableref{tbl:porter_algo}. The NLTK python library provides an updated implementation of the Porter's algorithm which can be used for stemming.

\begin{table}
    \centering
    \caption{Porter algorithm stemming rules}
    \label{tbl:porter_algo}
    \begin{tabular}{p{4cm}p{4cm}}
    \toprule
    Rule&Example \\
    \midrule
    (m$>$0) FUL -$>$  &  hopeful -$>$ hope \\
    (m$>$0) ICATE -$>$ IC  & triplicate -$>$ triplic \\
    (*v*) ED -$>$ & plastered -$>$ plaster \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{IDF Calculation}
The IDF values for the stems are calculated using Equation \eqref{eq:idf_equation}, which are then used to sort the stems in descending order of IDF values. The sorted list for the top 50 stems with their IDF values is shown in \tableref{tbl:idf_stems}. The table shows that the IDF value of stems such as "thief", "alcohol", "tragedi", "food", "trade", "citizen", "civilian", etc is pretty high. The presence of such stems in a text could be a strong indication of whether the text is significant or non-sginificant.

Since it might not be practical to use every single stem as a feature for the classifier, the number of stems should somehow be reduced. The simplest way to accomplish that task is to select the top $n$ stems for the sorted stem list. For this research, the top 1000 stems were used from the sorted stem list. The complete list of the selected stems is provided in \appendixref{1000_stems}. The list shows that the selected stems does include important stems like "murder", "robberi", "shoot", "burglari", "terror", "death", "victim" which is a desirable outcome.

\begin{table}
\begin{center}
\caption{Top 50 IDF stems}
\label{tbl:idf_stems}
\begin{tabular}{lc|cc|cr|}
\toprule
stem&IDF value&stem&IDF value&stem&IDF value\\
\midrule
graffiti&7.46&ring&7.33&harrow&7.33\\ \hline 
colour&7.33&mother&7.21&food&7.21\\ \hline 
tragedi&7.21&veteran&7.21&bay&7.21\\ \hline 
leg&7.21&tuozzolo&7.21&church&7.21\\ \hline 
design&7.21&termin&7.21&vote&7.21\\ \hline 
mornington&7.21&psos&7.21&brief&7.21\\ \hline 
babi&7.1&race&7.1&pull&7.1\\ \hline 
trade&7.1&post&7.1&coordin&7.1\\ \hline 
class&7.1&extra&7.1&complaint&7.1\\ \hline 
ps&7.1&came&7.1&worth&7.1\\ \hline 
teamwork&7.1&adult&7.1&civilian&7.1\\ \hline 
notic&7.1&collaps&7.1&brown&7.1\\ \hline 
greatest&7.1&terenc&7.1&count&7.1\\ \hline 
medic&7.1&plenti&7.1&akay&7.1\\ \hline 
one&7.1&citizen&7.1&noth&7.1\\ \hline 
strand&7.1&page&7.1&alcohol&7.1\\ \hline 
true&7.1&thief&7.1&&\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

% These stems that were selected are precisely the features that the classifier would use to assign the class to a text.

\subsection{TF-IDF}
Once the IDF values for the terms have been calculated, the TF-IDF algorithm can be used to convert the text into a vector by multiplying the TF of the term with its IDF value as shown in Equation \eqref{eq:tf_idf_equation}. The number of dimensions that the TF-IDF vector would have is precisely the number of stems selected as the vocabulary during the IDF calculation.

The vector $v$ obtained is then normalized using the $L^2$ norm as follows,
\begin{equation}
    v_{norm} = \frac{v}{\lVert v \rVert}
\end{equation}

The TF-IDF algorithm was implemented with the help of scikit-learn \cite{scikit-learn} python library which provides an interface for computing both the IDF value for the term and calculating the TF-IDF vector for a document.

% Classfication
\section{Classification}\label{classification}
Several classification algorithms can be used to classify the text such as rule-based classifiers, neural networks, logistic regression, random forest, decision trees, k-nearest neighbor, SVM\cite{cortes1995support}, etc. 
% However, it has been theoretically and empirically shown that SVMs are very well suited for the text categorization problem\cite{joachims1998text}. 
For this thesis, classifiers listed below were implemented using the scikit-learn python library.

\subsection{Models}
\subsubsection{Logistic Regression}
A logistic regression is a binary classification algorithm that tries to construct a decision boundary as one given in Equation \eqref{eq:regularized_logistic_regression} by minimizing a regularized cost for misclassification using a SGD algorithm.

\begin{equation}
    \label{eq:regularized_logistic_regression}
    h_{\theta}(x) = \frac{1}{1 + {e}^{-{\theta}^{t}. x}}
\end{equation}

\noindent
where $x$ is the input vector and $\theta$ is the vector containing the parameters of the decision boundary. 

\subsubsection{Support Vector Machine}
SVMs non-linearly map the input vectors to a very high dimensional feature space and tries to construct a linear decision surface in the new feature space \cite{cortes1995support}. The SVM could also use the kernel trick to construct a non-linear decision surface in the new feature space \cite{cortes1995support}. For the critical news detection task, the radial basis function shown in \eqref{eq:rbf_kernel} was used as a kernel:

\begin{equation}
    \label{eq:rbf_kernel}
    K(x, x^{'}) = exp(- \frac{{\lVert x - x^{'} \rVert}^{2}}{2 \sigma^{2}})
\end{equation}

\noindent
Where, $x$ and $x^{'}$ are the vector inputs, $d$ is the degree of the polynomial to use, and $\sigma$ is the smoothness parameter.

\subsubsection{Random Forests}
The random forests\cite{breiman2001random} is an ensemble method for decision trees\cite{quinlan1986induction} which can be used for both classification and regression analyses. The algorithm performs the prediction task by combining the output from independent tree predictors constructed during the training phase \cite{breiman2001random}. 

\subsubsection{Neural Network}
Neural networks are a networked learning model capable of performing both classification and regression tasks. The model hierarchically constructs a deeper representation of the given input and tries to perform the prediction task on the representation constructed. 

The Neural Network model for the critical news detection is a three layered network with a hidden layer, an input, and an output layer.

\subsection{Training and Testing}
%At first, the dataset is randomly separated into training set and test set using a 80-20 split. The models are trained using the training set whereas the test set remains unused during the training. 

For each classifier, the 5 fold cross-validation technique was used to evaluate the performance of the model using a random set of hyperparameters. The set of hyperparameters that achieved the highest performance during the cross-validation were selected, and the models were then trained on the whole training set using the selected hyperparameters.

In order to evaluate the performance, the trained models were used to predict the class assignment for news articles in the test set. 

\section{Summary}
In summary, this chapter describes the steps that were used to construct and evaluate the classifier models. The chapter explains the process that was used to collect, analyze, and store the data. It then describes the algorithms and the libraries that were used to obtain features from a text, which is followed by the methodology that was used to train and test the classifiers.