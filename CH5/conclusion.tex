\chapter{Conclusion}\label{conclusion}
The key idea that this thesis explores is that for fake news detection there are articles on topics such as infotainment, personal news, health tips, etc which are somewhat unimportant so it is not necessary to verify the truthfulness of such articles.

In order to formalize the problem better, a new term "significant news" was defined as:

\enquote{A text is labeled as significant if it affects significant number of people, changes the routines of daily life, and needs verification on the information presented.}

The goal of the thesis was to evaluate if it was possible to construct a classifier that was able to separate a significant news story from a non-significant news story.

A new dataset was created by manually labelling the posts of several Twitter accounts including NYPD News, Metropolitan Police, Victoria Police, Seattle Police Dept, and NYPDCounterterrorism as either significant or as non-significant. The result of that effort was a dataset with 1548 significant and 595 non-significant Tweets.

The TF-IDF algorithm was then used to convert text to a 1000 dimensional vector that would capture the distinguishing features of the text. The vectors were then used to train and evaluate the performance of multiple classifiers: regularized logistic regression, random forests, support vector machine, and neural network. It was observed that the classifiers performed reasonably well on the significant news detection task. The models on average were able to achieve a test accuracy greater than 90 percent with the Neural Network model achieving the high test accuracy of 93.654 percent.

Thus, the results from the experiments indicate that it is possible to construct quite an accurate classifier for separating significant news from non-significant news. Once the classifier is trained, it can be used either as a standalone tool with human reviewers to detect fake news, or it could be used with another automated fake news classifier to filter significant news that should be verified for its truthfulness.

\section{Future Works}
There are several possibilities that could be explored to further improve the performance of the classifier. The first step would be to refine the data and extend it if possible. Adding more samples to the dataset and eliminating noise generally improves the performance of most classifiers, which should be the case here as well.

We have put significant effort in building the dataset and labeling it. There is still vagueness in the definition of significant news. For example, the significant news should affect a large number of people. The number of people is vague in the definition. As future work, a survey may be conducted and labeling could be done based on survey results or using crowdsourcing. 

Another possibility that could be explored is to use a better feature representation of the text. The TF-IDF feature weighing scheme could be extended to other n-gram models such as bigram, trigram, etc to obtain the vector representation of the text. Using higher n-gram models could provide better representation of the text which could improve the performance. The feature representation of the text could also be improved by using the word embedding models. It has been empirically shown that most of the existing NLP systems can be improved by using word-embedding as an extra feature\cite{turian2010word}. However, the extent of improvement that word-embedding models could bring about for significant news detection remains to be seen. In addition, the size of feature vector could be reduced to see whether it affects the performance or not. Moreover, tokenization can be reevaluated to maintain the temporal context of the messages. 

Yet another possibility that could be explored is replacing the existing classifiers with sequence analysis algorithms such as HMM\cite{baum1966statistical}, LSTM\cite{hochreiter1997long}, or GRU\cite{cho2014learning}. Such algorithms could improve the prediction performance by being able to construct an internal representation of the text that captures its context and meaning.
