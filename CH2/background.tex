\chapter{Background} \label{background}
The goal of text classification is to automatically assign a document to one of the pre-defined classes using the information available from the content and the meta-data of the document.

For most text processing systems, the stream of character representation of the text needs to be converted to another suitable representation that can be used with the analysis algorithms. A popular way of representing a text is to convert it into vectors that somehow capture the essence of the document. 

The vectors can then be used to construct predictive models that can assign class labels to the documents. The complexity of such models could range from simple hand-crafted rule-based classifiers to models that can learn from a labelled dataset. Furthermore, the label assignment itself could be categorized as hard or soft based on whether the assignment is explicit -- assigning the label itself to the text -- or implicit -- assigning a probability value. 

\section{Feature Generation}
\subsection{Tokenization}
Tokenization is the first step in text processing which converts the stream of characters representation of the text to a sequence of words by removing the white space characters and punctuation symbols between the words. An example of tokenization on a given text is shown in \tableref{tbl:tokenization_example}.

\begin{table}[h]
\begin{center}
\caption{Example of tokenization}
\label{tbl:tokenization_example}
\begin{tabular}{p{6cm}p{6cm}}
\toprule 
Text&Tokens\\
\midrule 
``Don't go there."&[``Don't", ``go", ``there"]\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Stop word filtering}
Words such as ``a", ``the", ``you", etc. are very common across English text documents, so the occurrence of these words is a poor indicator of whether the text belongs to a particular class. A common practice is to filter out these words for the feature set, and it has been empirically shown that filtering such words improves the performance on most text classification tasks \cite{silva2003importance}. These words that are filtered out during the processing are known as stop words. 

\subsection{Stemming}
Stemming is the process of reducing the inflected words to their stem or root so that they represent the same feature during the processing. For example, the word ``happiness" can be reduced to its stem ``happy", and the word ``loved" can also be reduced to its stem ``love". By reducing the words to their stems, not only decreases the number of features, but also improves the feature set itself  as the inflected words of the same stem will be considered as a single feature instead of multiple features. 

\subsection{Vector Space Model}
The vector space model (VSM) or bag-of-words model represents a text as a collection of words without regards to the sequence in which they occur. In VSM, each unique word in the vocabulary is treated as a feature of the text because of which it usually ends up with a large number of features. For such cases, the feature extraction algorithms such as information gain, gini index, latent semantic indexing, linearized singular value decomposition, etc. can be used for selecting the relevant features from the original feature set \cite{aggarwal2012survey}.

\subsection{n-gram models}
The n-gram model of a text is a probabilistic model that represents text as a continuous sequence of $n$ words in a such way that the context of the word is captured along with the word itself. The VSM is a special case of the n-gram model where the value of $n$ is 1. Some of the n-gram representation of the text ``The quick brown fox jumps." is shown in \tableref{tbl:ngram_representation}.

\begin{table}[h]
\begin{center}
\caption{Example ngram representation}
\label{tbl:ngram_representation}
\begin{tabular}{p{3cm}p{8cm}}
\toprule 
Model&Representation\\
\midrule 
Unigram($n=1$)& ``The", ``quick", ``brown", ``fox", ``jumps" \\
Bigram($n=2$)& ``The quick", ``quick brown", ``brown fox", ``fox jumps" \\
Trigram($n=3$)& ``The quick brown", ``quick brown fox", ``brown fox jumps" \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{TF-IDF}
TF-IDF\cite{sparck1972statistical} is a rather simple feature weighing scheme that assigns some weight to a term based on its occurrence in the text and its importance in the corpus. TF-IDF for a term $t$ in a document $d$ is calculated as a product of two terms: Term Frequency ($tf$) and Inverse Document Frequency ($idf$). 
\begin{equation}
    \label{eq:tf_idf_equation}
    tfidf(t, d) = tf(t, d) * idf(t)
\end{equation}

The $tf$ for a term in a document is the count of the term in the document and $idf$ for a term is calculated as follows:
\begin{equation}
    \label{eq:idf_equation}
    idf(t) = \log{\frac{N}{1 + df(t)}}
\end{equation}

\noindent
where $N$ is the number of total number of documents and $df(t)$ is the number of documents that contains the term $t$.

\subsection{Word Embedding Models}
Lately, there has been a significant interest and development in word embedding models \cite{mikolov2013distributed, pennington2014glove, bojanowski2017enriching}. In a word embedding model, each word is represented by a vector in a feature space such that similar words will be represented by similar vectors\cite{mikolov2013distributed}. The word vectors are then combined to create a vector representation of the text which by definition will force two texts that are similar in content but different in words to have a similar vector representation.

% Related Work
\section{Related Work on Classifying Text} \label{related_works}
Due to the way the significant news is defined in this thesis, there is not much research work that could be used for direct comparison. The closest classification that could be compared is the dichotomy of hard/soft news but the hard/soft classification of news is mostly confined to journalism and an actual implementation of such classifiers is rarely found. The majority of research instead focuses on the much broader problem of topic classification for various purposes \cite{wang2012baselines, lee2011twitter, joachims1998text, nigam2000text}.

The topic classification algorithm could be used for hard/soft news classification by using the labelling technique shown in \tableref{tbl:topics_to_hard_soft}. However, the soundness of such a method for hard/soft news classification remains unknown for now.

\begin{table}[h]
\begin{center}
\caption{Conversion of topic labels to hard/soft news labels}
\label{tbl:topics_to_hard_soft}
\begin{tabular}{lr}
\toprule 
Topics&Label \\
\midrule 
politics, public administration, science, technology, etc & Hard\\
\hline
celebrities, human interest, sport, entertainment, etc & Soft\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Lately, the fake news detection problem has received a lot of interest from researchers. A fake news detection algorithm could use any combination of information among news content, social content (e.g. user's engagement), and dynamic information (e.g. news propagation) in order to perform the prediction \cite{shu2018fakenewsnet}; however, even with a multitude of such information, it is not easy to construct an automated algorithm for fake news detection\cite{shu2017fake}.

One of the simplest models that was successfully implemented for fake news detection is the naive bayes classifier which was trained on a modified BuzzFeedNews\footnote{\url{https://github.com/BuzzFeedNews/2016-10-facebook-fact-check}} dataset containing Facebook posts of news articles \cite{granik2017fake}. The original dataset consists of four labels for truthfulness rating which are distributed as shown in \tableref{tbl:buzzfeed_dataset}. In the research, the number of classes were reduced to two by discarding articles which were labelled as ``mixture of true and false" or ``no factual content" along with articles without any text in them. The classifier was then trained using the remaining 1771 news articles for the binary classification problem. The model was then able to achieve a test accuracy of 74 percent which is a good result given the simplicity of the model.

\begin{table}[h]
\begin{center}
\caption{Statistics about BuzzFeed dataset}
\label{tbl:buzzfeed_dataset}
\begin{tabular}{lr}
\toprule 
Label&Number of samples \\
\midrule 
mostly false&104 \\
mixture of true and false&245 \\
mostly true&1669 \\
no factual content&264 \\
\midrule
&2282 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The research by Gilda, 2017 provides the most exhaustive performance comparison of standard classification algorithms for the fake news detection problem \cite{gilda2017evaluating}. The research contrasts the performance of various classification algorithms  (support vector machines (SVM), stochastic gradient descent (SGD), random forest, bounded decision trees, and gradient boosting) by training them on a modified Signal Media news dataset \cite{corney2016million} which has labeled articles from verified reliable sources as truthful and articles from verified unreliable sources as fake. The distribution of the labelled dataset that was obtained from the original unlabelled 1 million articles is shown in \tableref{tbl:reduced_signal_media_dataset}. The performance comparison of the standard classifiers on the new dataset when using a TF-IDF as the feature set is shown in \tableref{tbl:signal_media_performance} \cite{gilda2017evaluating}. It empirically shows that most of the standard classifiers work reasonably well for fake news detection on the Signal Media dataset.

\begin{table}[h]
\begin{center}
\caption{Reduced Signal Media dataset for Fake News Detection}
\label{tbl:reduced_signal_media_dataset}
\begin{tabular}{lr}
\toprule 
Label&Number of samples \\
\midrule 
fake&3217 \\
truthful&7834 \\
\midrule
&11051 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
\caption{Performance of models in reduced Signal Media dataset}
\label{tbl:signal_media_performance}
\begin{tabular}{lr}
\toprule 
Model&Test Accuracy\\
\midrule 
Naive Bayes&67.89\\
Bounded Decision Trees&66.1\\
Gradient Boosting&68.7\\
Random Forests&67.6\\
Stochastic Gradient Descent&77.2\\
Support Vector Machine&76.2\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

However, for the LIAR\cite{wang2017liar} dataset the performance of the standard classifiers are a bit underwhelming as shown in \tableref{tbl:liar_performance} \cite{wang2017liar}.  Note that this is a multi-class classification rather than a binary classification due to the number of labels in the dataset. Nevertheless, the accuracy is not impressive and this could be due to the dataset.  The LIAR dataset is human-labeled short statements obtained from POLITIFACT\footnote{\url{https://www.politifact.com/}} API which contains six labels assigned based on their truthfulness rating: ``pants-fire", ``false", ``barely-true", ``half-true", ``mostly-true", and ``true". The distribution of the labels in the dataset is mostly balanced except for 1050 ``pants-fire" cases while for all other labels it ranges from 2063 to 2638. Since the articles are short and require fact-checking to determine their truthfulness, it is much harder for the classifier to perform the prediction.

\begin{table}[h]
\begin{center}
\caption{Performance of models in LIAR dataset}
\label{tbl:liar_performance}
\begin{tabular}{lccc}
\toprule 
Model&Test Accuracy\\
\midrule 
Logistic Regression&24.7\\
SVM&25.5\\
Bidirectional LSTM&23.3\\
CNN&27.0\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Unsatisfactory performance of detecting fake news solely from news content has led researchers to investigate the relation between users on social media and news \cite{shu2018understanding}. The research suggested that the features of users who were more likely to trust fake news over real news were different from those of users who were more likely to trust real news. The TCNN-URG model for fake news detection by Qian et. al., 2018 was able to use this information to its advantage. The model combined the information from news content with dynamic information (historical responses by the user) of the article to perform the prediction. The performance of the model was then compared with standard classification techniques by training them on the Weibo dataset\cite{ma2016detecting} which consisted of 2313 rumors and 2351 non-rumor articles obtained using the API of Sina Weibo. The performance achieved by the models is shown in \tableref{tbl:weibo_performance} \cite{qian2018neural}. It was observed that TCNN-URG model performed well compared to the previous models that only utilized the information from news content.

\begin{table}[h]
\begin{center}
\caption{Performance of models in Weibo dataset}
\label{tbl:weibo_performance}
\begin{tabular}{lccc}
\toprule 
\rule[-1pt]{0pt}{14pt}Model&Test Accuracy\\
\midrule 
SVM with LIWC&66.06\\
SVM with POS-gram&74.77\\
SVM with 1-gram&84.76\\
CNN&86.23\\
TCNN&88.08\\
TCNN-URG&89.84\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

One study that tries to tackle fake news detection from an entirely different perspective is the research by Chen et. al, 2015. The research starts with the fact that news with clickbait headlines often had content which were misleading and unverified \cite{silverman2016analysis}. Using that premise with the fact that news with clickbait headlines had been the major factor responsible for the spread of fake news \cite{silverman2015lies}. The research proposed a clickbait article detection system as a means of identifying the fake news \cite{chen2015misleading}.

\section{Summary}
This chapter provides a brief explanation of text classification and explains the concepts related to text processing: tokenization, stop words filtering, stemming, vector space model, n-gram model, TF-IDF, and word embedding models. The chapter then describes various related research done in the field of fake news detection.
